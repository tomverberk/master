{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6: Bayesian models\n",
    "We will first learn a GP regressor for an artificial, non-linear function to illustrate some basic aspects of GPs. To this end, we consider a sinusoidal function from which we sample a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to predict and the dataset we create from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    return np.sin((x - 2.5) ** 3)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "t = np.linspace(0,5,1000)\n",
    "plt.plot(t, f(t), 'r', label = 'original f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we create based on the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sampled from a sine function\n",
    "rng = np.random.RandomState(4)\n",
    "X_ = rng.uniform(0, 5, 1000)[:, np.newaxis]\n",
    "y_ = f(X_).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(g, X_train, y_train, X_full, y_full, y_pred_mean, y_pred_std, use_title=\"yes\"):\n",
    "    \"\"\"\n",
    "    Visualizes the GP predictions, training points and original function\n",
    "    \n",
    "    Attributes:\n",
    "    X_train -- The training data\n",
    "    y_train -- The correct labels\n",
    "    X_full -- The data to calculate predictions\n",
    "    y_full -- The correct labels of the prediction data\n",
    "    y_pred_mean -- the predicted means\n",
    "    y_pred_std -- the predicted st. devs.\n",
    "    \"\"\"\n",
    "    x_ = np.linspace(0, 5, 1000)[:,np.newaxis]\n",
    "    \n",
    "    idx = np.argsort(X_full[:,0])\n",
    "    \n",
    "    # Original function\n",
    "    a = X_full[idx]\n",
    "    b = y_full[idx]\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(a, \n",
    "             b, 'r', label = 'original f(x)')\n",
    "    \n",
    "    # Training points\n",
    "    plt.scatter(X_train, y_train, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))\n",
    "    \n",
    "    # Prediction \n",
    "    d = y_pred_mean[idx]\n",
    "    e = y_pred_std[idx]\n",
    "    plt.plot(a, d, 'k', lw=1, zorder=9)\n",
    "    plt.fill_between(a[:,0], d - 1.96*e, d + 1.96*e, alpha=0.2, color='k')\n",
    "    \n",
    "    if use_title == \"yes\":\n",
    "        plt.title(\"Posterior (kernel: %s)\\n Log-Likelihood: %.3f\"\n",
    "              % (g.kernel_, g.log_marginal_likelihood(g.kernel_.theta)),\n",
    "              fontsize=12)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: visualizing predictions\n",
    "\n",
    "Train a GP regressor with a RBF kernel with default hyperparameters on a 1% sample of the sine data. Note that by learning a GP the hyperparameters of the chosen kernel are tuned automatically. To visualize what the GP has learned, use the model to predict values for the entire dataset. Plot the original function, the predictions and the training data points. You can use the function `plot_gp()` to assist with plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: reducing the uncertainty\n",
    "Fit a model using 5% and 10% of the data. Now try setting `n_restarts_optimizer` in the `GaussianProcessRegressor` constructor. Plot the results. What differences do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Kernels\n",
    "Like SVMs, kernels play a major role in GPs. Using a 5% sample of the data, train one GP  with each of the following kernels:\n",
    "    \n",
    "* RBF\n",
    "* RationalQuadratic\n",
    "* ExpSineSquared\n",
    "* DotProduct\n",
    "* Matern\n",
    "\n",
    "What differences do you see in the log-likelihood? Which model fit best the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Mauna Loa data\n",
    "We now look at the problem of predicting the monthly average CO2 concentrations collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. This is a time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# originally from sci-kit learn\n",
    "def load_mauna_loa_atmospheric_co2():\n",
    "    ml_data = fetch_openml(data_id=41187, as_frame=False)\n",
    "    months = []\n",
    "    ppmv_sums = []\n",
    "    counts = []\n",
    "\n",
    "    y = ml_data.data[:, 0]\n",
    "    m = ml_data.data[:, 1]\n",
    "    month_float = y + (m - 1) / 12\n",
    "    ppmvs = ml_data.target\n",
    "\n",
    "    for month, ppmv in zip(month_float, ppmvs):\n",
    "        if not months or month != months[-1]:\n",
    "            months.append(month)\n",
    "            ppmv_sums.append(ppmv)\n",
    "            counts.append(1)\n",
    "        else:\n",
    "            # aggregate monthly sum to produce average\n",
    "            ppmv_sums[-1] += ppmv\n",
    "            counts[-1] += 1\n",
    "\n",
    "    months = np.asarray(months).reshape(-1, 1)\n",
    "    avg_ppmvs = np.asarray(ppmv_sums) / counts\n",
    "    return months, avg_ppmvs\n",
    "\n",
    "\n",
    "X_mauna, y_mauna = load_mauna_loa_atmospheric_co2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick visualization\n",
    "plt.plot(X_mauna,y_mauna)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('co2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1\n",
    "Signals like this usually consist of a combination of different \"sub-signals\", e.g. a long-term component, a seasonal component, a noise component, and so on. When defining a GP kernel, you can combine multiple kernels, such as:\n",
    "\n",
    "* A RBF kernel can be used to explain long-term, smooth patterns.\n",
    "* The seasonal component can be modeled by an `ExpSineSquared` component.\n",
    "* Small and medium-term irregularities can be modeled by a `RationalQuadratic` component.\n",
    "* `WhiteNoise` kernel to model white noise.\n",
    "\n",
    "Train a GP using the first 75% data points as training data using the kernel below. Experiment with removing one or more kernels and check the results visually (you can use `plot_gp`). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend\n",
    "k2 = 2.0**2 * RBF(length_scale=100.0) \\\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=1.0,\n",
    "                     periodicity_bounds=\"fixed\")  # seasonal component\n",
    "# medium term irregularities\n",
    "k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "k4 = 0.1**2 * RBF(length_scale=0.1) \\\n",
    "    + WhiteKernel(noise_level=0.1**2,\n",
    "                  noise_level_bounds=(1e-5, np.inf))  # noise terms\n",
    "\n",
    "kernel = k1 + k2 +  k3 + k4"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
